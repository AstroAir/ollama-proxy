{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Ollama Proxy","text":"<p>A modern, high-performance proxy server that translates Ollama API calls to OpenRouter, enabling seamless access to a wide variety of AI models through the familiar Ollama interface.</p> <p> </p>"},{"location":"#overview","title":"Overview","text":"<p>Ollama Proxy acts as a bridge between any Ollama-compatible client and the OpenRouter API. This allows you to use your favorite tools and applications that support Ollama with the extensive range of models offered by OpenRouter, without needing to modify your client-side code.</p>"},{"location":"#features","title":"Features","text":"<ul> <li>\ud83d\udd04 Seamless Translation: Converts Ollama API calls to the OpenRouter format.</li> <li>\ud83d\ude80 High Performance: Built with modern Python and <code>asyncio</code> for speed.</li> <li>\u2699\ufe0f Flexible Configuration: Configure via environment variables, <code>.env</code> files, or CLI arguments.</li> <li>\ud83d\udd0d Model Filtering: Control which OpenRouter models are exposed.</li> <li>\ud83d\udcca Structured Logging: JSON logs for better observability.</li> <li>\ud83d\udc33 Docker Support: Easy to deploy with Docker and Docker Compose.</li> </ul>"},{"location":"#quick-start","title":"Quick Start","text":""},{"location":"#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.11+</li> <li>An OpenRouter API key</li> </ul>"},{"location":"#installation","title":"Installation","text":"<ol> <li> <p>Clone the repository: <pre><code>git clone https://github.com/your-username/ollama-proxy.git\ncd ollama-proxy\n</code></pre></p> </li> <li> <p>Install dependencies: <pre><code>pip install -e .\n</code></pre></p> </li> <li> <p>Configure your API key:     Create a <code>.env</code> file and add your key:     <pre><code>OPENROUTER_API_KEY=\"your_openrouter_api_key_here\"\n</code></pre></p> </li> <li> <p>Run the server: <pre><code>ollama-proxy\n</code></pre></p> </li> </ol> <p>Once the server is running, you can configure your Ollama client to point to <code>http://localhost:11434</code> (or your custom host and port).</p>"},{"location":"#documentation","title":"Documentation","text":"<p>Explore our comprehensive documentation to learn more about configuring and using the Ollama Proxy:</p> <ul> <li>Configuration Guide: Learn how to customize the proxy's behavior.</li> <li>API Reference: See the full list of supported Ollama API endpoints.</li> <li>Usage Examples: Practical examples of how to use the proxy with various tools.</li> <li>Deployment Guide: Find out how to deploy the proxy in a production environment.</li> <li>Architecture Overview: Get a deeper understanding of the project's design.</li> <li>Troubleshooting Guide: Find solutions to common problems.</li> </ul>"},{"location":"#contributing","title":"Contributing","text":"<p>Contributions are welcome! Please see the Contributing Guide for details on how to contribute to the project.</p>"},{"location":"#license","title":"License","text":"<p>This project is licensed under the MIT License. See the LICENSE file for details.</p>"},{"location":"api-reference/","title":"API Reference","text":"<p>This document provides a detailed reference for the Ollama API endpoints supported by the Ollama Proxy. The proxy aims to be a drop-in replacement for the official Ollama server, so it maintains compatibility with the most common endpoints.</p>"},{"location":"api-reference/#general-information","title":"General Information","text":"<ul> <li>Base URL: <code>http://&lt;host&gt;:&lt;port&gt;</code></li> <li>Default Port: <code>11434</code></li> <li>Authentication: All requests require a valid OpenRouter API key configured on the proxy server</li> <li>Content-Type: All POST requests should use <code>Content-Type: application/json</code></li> </ul>"},{"location":"api-reference/#supported-endpoints","title":"Supported Endpoints","text":""},{"location":"api-reference/#health-monitoring","title":"Health &amp; Monitoring","text":""},{"location":"api-reference/#get","title":"<code>GET /</code>","text":"<p>Returns a simple health check message to confirm that the server is running.</p> <ul> <li>Success Response (200 OK):     <pre><code>Ollama is running\n</code></pre></li> </ul>"},{"location":"api-reference/#get-apiversion","title":"<code>GET /api/version</code>","text":"<p>Returns the version of the proxy.</p> <ul> <li>Success Response (200 OK):     <pre><code>{\n  \"version\": \"0.1.0-openrouter\"\n}\n</code></pre></li> </ul>"},{"location":"api-reference/#get-health","title":"<code>GET /health</code>","text":"<p>Returns detailed health information about the proxy.</p> <ul> <li>Success Response (200 OK):     <pre><code>{\n  \"status\": \"healthy\",\n  \"uptime_seconds\": 1234.56,\n  \"model_count\": 42,\n  \"filtered_model_count\": 10,\n  \"request_count\": 123,\n  \"error_count\": 0,\n  \"error_rate\": 0.0,\n  \"last_model_refresh\": 1640995200.0,\n  \"environment\": \"production\"\n}\n</code></pre></li> </ul>"},{"location":"api-reference/#get-metrics","title":"<code>GET /metrics</code>","text":"<p>Returns metrics for monitoring and observability.</p> <ul> <li>Success Response (200 OK):     <pre><code>{\n  \"metrics\": [...],\n  \"statistics\": {...},\n  \"timestamp\": 1640995200.0\n}\n</code></pre></li> </ul>"},{"location":"api-reference/#model-management","title":"Model Management","text":""},{"location":"api-reference/#get-apitags","title":"<code>GET /api/tags</code>","text":"<p>Lists all available models that are accessible through the proxy. The list is fetched from OpenRouter and can be filtered using the model filter configuration.</p> <ul> <li>Success Response (200 OK):     <pre><code>{\n  \"models\": [\n    {\n      \"name\": \"google/gemini-pro:latest\",\n      \"modified_at\": \"2023-12-12T14:00:00Z\",\n      \"size\": 7000000000,\n      \"digest\": \"sha256:abcdef1234567890\",\n      \"details\": {\n        \"format\": \"gguf\",\n        \"family\": \"gemini\",\n        \"families\": [\"gemini\"],\n        \"parameter_size\": \"7B\",\n        \"quantization_level\": \"Unknown\"\n      }\n    }\n  ]\n}\n</code></pre></li> </ul>"},{"location":"api-reference/#post-apishow","title":"<code>POST /api/show</code>","text":"<p>Provides detailed information about a specific model. Note that much of the information is stubbed since it is not available from the OpenRouter API.</p> <ul> <li>Request Body:     <pre><code>{\n  \"name\": \"google/gemini-pro:latest\"\n}\n</code></pre></li> <li>Success Response (200 OK):     <pre><code>{\n  \"license\": \"\",\n  \"modelfile\": \"\",\n  \"parameters\": \"\",\n  \"template\": \"\",\n  \"details\": {\n    \"parent_model\": \"\",\n    \"format\": \"\",\n    \"family\": \"gemini\",\n    \"families\": [\"gemini\"],\n    \"parameter_size\": \"Unknown\",\n    \"quantization_level\": \"\"\n  },\n  \"model_info\": {},\n  \"tensors\": []\n}\n</code></pre></li> </ul>"},{"location":"api-reference/#inference","title":"Inference","text":""},{"location":"api-reference/#post-apichat","title":"<code>POST /api/chat</code>","text":"<p>Handles chat completion requests. This is the primary endpoint for interacting with models.</p> <ul> <li>Request Body:     <pre><code>{\n  \"model\": \"google/gemini-pro:latest\",\n  \"messages\": [\n    {\n      \"role\": \"system\",\n      \"content\": \"You are a helpful assistant.\"\n    },\n    {\n      \"role\": \"user\",\n      \"content\": \"Why is the sky blue?\"\n    }\n  ],\n  \"stream\": false,\n  \"options\": {\n    \"temperature\": 0.7,\n    \"top_p\": 0.9\n  }\n}\n</code></pre></li> <li>Response (Non-streaming):     <pre><code>{\n  \"model\": \"google/gemini-pro:latest\",\n  \"created_at\": \"2023-12-12T14:00:00Z\",\n  \"message\": {\n    \"role\": \"assistant\",\n    \"content\": \"The sky is blue because of Rayleigh scattering...\"\n  },\n  \"done\": true,\n  \"total_duration\": 0,\n  \"load_duration\": 0,\n  \"prompt_eval_count\": null,\n  \"prompt_eval_duration\": 0,\n  \"eval_count\": 0,\n  \"eval_duration\": 0\n}\n</code></pre></li> <li>Response (Streaming): A stream of JSON objects, each representing a token or a final summary.     <pre><code>{\"model\":\"google/gemini-pro:latest\",\"created_at\":\"2023-12-12T14:00:00Z\",\"message\":{\"role\":\"assistant\",\"content\":\"The\"},\"done\":false}\n{\"model\":\"google/gemini-pro:latest\",\"created_at\":\"2023-12-12T14:00:00Z\",\"message\":{\"role\":\"assistant\",\"content\":\" sky\"},\"done\":false}\n{\"model\":\"google/gemini-pro:latest\",\"created_at\":\"2023-12-12T14:00:00Z\",\"message\":{\"role\":\"assistant\",\"content\":\" is\"},\"done\":false}\n{\"model\":\"google/gemini-pro:latest\",\"created_at\":\"2023-12-12T14:00:00Z\",\"message\":{\"role\":\"assistant\",\"content\":\" blue\"},\"done\":false}\n{\"model\":\"google/gemini-pro:latest\",\"created_at\":\"2023-12-12T14:00:00Z\",\"message\":{\"role\":\"assistant\",\"content\":\" because\"},\"done\":false}\n{\"model\":\"google/gemini-pro:latest\",\"created_at\":\"2023-12-12T14:00:00Z\",\"message\":{\"role\":\"assistant\",\"content\":\" of\"},\"done\":false}\n{\"model\":\"google/gemini-pro:latest\",\"created_at\":\"2023-12-12T14:00:00Z\",\"message\":{\"role\":\"assistant\",\"content\":\" Rayleigh\"},\"done\":false}\n{\"model\":\"google/gemini-pro:latest\",\"created_at\":\"2023-12-12T14:00:00Z\",\"message\":{\"role\":\"assistant\",\"content\":\" scattering\"},\"done\":false}\n{\"model\":\"google/gemini-pro:latest\",\"created_at\":\"2023-12-12T14:00:00Z\",\"message\":{\"role\":\"assistant\",\"content\":\".\"},\"done\":true}\n</code></pre></li> </ul>"},{"location":"api-reference/#post-apigenerate","title":"<code>POST /api/generate</code>","text":"<p>Handles text generation requests (a simpler version of <code>/api/chat</code>).</p> <ul> <li>Request Body:     <pre><code>{\n  \"model\": \"google/gemini-pro:latest\",\n  \"prompt\": \"Once upon a time\",\n  \"system\": \"You are a creative writer.\",\n  \"stream\": false,\n  \"options\": {\n    \"temperature\": 0.8\n  }\n}\n</code></pre></li> <li>Response (Non-streaming):     <pre><code>{\n  \"model\": \"google/gemini-pro:latest\",\n  \"created_at\": \"2023-12-12T14:00:00Z\",\n  \"response\": \" there was a brave knight...\",\n  \"done\": true,\n  \"context\": [],\n  \"total_duration\": 0,\n  \"load_duration\": 0,\n  \"prompt_eval_count\": null,\n  \"prompt_eval_duration\": 0,\n  \"eval_count\": 0,\n  \"eval_duration\": 0\n}\n</code></pre></li> <li>Response (Streaming): A stream of JSON objects.     <pre><code>{\"model\":\"google/gemini-pro:latest\",\"created_at\":\"2023-12-12T14:00:00Z\",\"response\":\" there\",\"done\":false}\n{\"model\":\"google/gemini-pro:latest\",\"created_at\":\"2023-12-12T14:00:00Z\",\"response\":\" was\",\"done\":false}\n{\"model\":\"google/gemini-pro:latest\",\"created_at\":\"2023-12-12T14:00:00Z\",\"response\":\" a\",\"done\":false}\n{\"model\":\"google/gemini-pro:latest\",\"created_at\":\"2023-12-12T14:00:00Z\",\"response\":\" brave\",\"done\":false}\n{\"model\":\"google/gemini-pro:latest\",\"created_at\":\"2023-12-12T14:00:00Z\",\"response\":\" knight\",\"done\":false}\n{\"model\":\"google/gemini-pro:latest\",\"created_at\":\"2023-12-12T14:00:00Z\",\"response\":\"...\",\"done\":true}\n</code></pre></li> </ul>"},{"location":"api-reference/#embeddings","title":"Embeddings","text":""},{"location":"api-reference/#post-apiembed-and-post-apiembeddings","title":"<code>POST /api/embed</code> and <code>POST /api/embeddings</code>","text":"<p>Generates embeddings for a given input. Both endpoints are supported for compatibility.</p> <ul> <li>Request Body for <code>/api/embed</code>:     <pre><code>{\n  \"model\": \"text-embedding-ada-002\",\n  \"input\": \"This is a test sentence.\"\n}\n</code></pre></li> <li>Request Body for <code>/api/embeddings</code>:     <pre><code>{\n  \"model\": \"text-embedding-ada-002\",\n  \"prompt\": \"This is a test sentence.\"\n}\n</code></pre></li> <li>Success Response (200 OK):     <pre><code>{\n  \"embedding\": [0.1, 0.2, 0.3, ...]\n}\n</code></pre></li> </ul>"},{"location":"api-reference/#process-management","title":"Process Management","text":""},{"location":"api-reference/#get-apips","title":"<code>GET /api/ps</code>","text":"<p>Lists running models (stubbed implementation).</p> <ul> <li>Success Response (200 OK):     <pre><code>{\n  \"models\": [],\n  \"created_at\": \"2023-12-12T14:00:00Z\"\n}\n</code></pre></li> </ul>"},{"location":"api-reference/#error-responses","title":"Error Responses","text":"<p>The proxy returns standardized error responses for various conditions:</p>"},{"location":"api-reference/#model-not-found-404","title":"Model Not Found (404)","text":"<pre><code>{\n  \"error\": \"Model 'nonexistent-model' not found.\",\n  \"type\": \"model_not_found\"\n}\n</code></pre>"},{"location":"api-reference/#model-forbidden-403","title":"Model Forbidden (403)","text":"<pre><code>{\n  \"error\": \"Model 'forbidden-model' is not allowed by the filter.\",\n  \"type\": \"model_forbidden\"\n}\n</code></pre>"},{"location":"api-reference/#openrouter-api-error-502","title":"OpenRouter API Error (502)","text":"<pre><code>{\n  \"error\": \"OpenRouter API error: 401 Unauthorized\",\n  \"type\": \"openrouter_error\"\n}\n</code></pre>"},{"location":"api-reference/#internal-server-error-500","title":"Internal Server Error (500)","text":"<pre><code>{\n  \"error\": \"Internal server error\",\n  \"type\": \"internal_error\"\n}\n</code></pre>"},{"location":"api-reference/#unsupported-endpoints","title":"Unsupported Endpoints","text":"<p>The following Ollama API endpoints are not supported by the proxy and will return an <code>HTTP 501 Not Implemented</code> error:</p> <ul> <li><code>POST /api/create</code></li> <li><code>POST /api/copy</code></li> <li><code>DELETE /api/delete</code></li> <li><code>POST /api/pull</code></li> <li><code>POST /api/push</code></li> <li><code>POST /api/blobs/{digest}</code></li> <li><code>HEAD /api/blobs/{digest}</code></li> </ul> <p>These endpoints are related to local model management, which is not applicable when using the OpenRouter proxy.</p>"},{"location":"architecture/","title":"Architecture Overview","text":"<p>This document provides a deep dive into the architecture of the Ollama Proxy, its key components, and the design principles behind it.</p>"},{"location":"architecture/#project-structure","title":"Project Structure","text":"<p>The project is organized into two main directories: <code>src</code> for the application's source code and <code>tests</code> for the test suite.</p> <pre><code>src/\n\u251c\u2500\u2500 __init__.py\n\u251c\u2500\u2500 main.py              # Entry point and CLI\n\u251c\u2500\u2500 app.py               # FastAPI application factory\n\u251c\u2500\u2500 config.py            # Configuration management\n\u251c\u2500\u2500 models.py            # Pydantic models and schemas\n\u251c\u2500\u2500 api.py               # API route handlers\n\u251c\u2500\u2500 openrouter.py        # OpenRouter client\n\u251c\u2500\u2500 utils.py             # Utility functions\n\u251c\u2500\u2500 exceptions.py        # Custom exceptions\n\u251c\u2500\u2500 logging_config.py    # Logging configuration\n\u2514\u2500\u2500 monitoring.py        # Monitoring and metrics collection\n</code></pre>"},{"location":"architecture/#key-components","title":"Key Components","text":""},{"location":"architecture/#1-fastapi-application-apppy","title":"1. FastAPI Application (<code>app.py</code>)","text":"<p>The core of the proxy is a FastAPI application. FastAPI was chosen for its high performance, asynchronous support, and automatic generation of interactive API documentation.</p> <p>The <code>create_app</code> function in <code>app.py</code> is a factory that initializes and configures the FastAPI application, including setting up logging, state management, and API routers.</p> <p>Key features of the FastAPI application: - Lifespan Management: Uses FastAPI's lifespan feature to initialize resources on startup and clean them up on shutdown - Middleware: Includes custom middleware for request logging and error handling - Exception Handlers: Global exception handlers for consistent error responses - CORS Support: Cross-Origin Resource Sharing support for web client integration</p>"},{"location":"architecture/#2-configuration-configpy","title":"2. Configuration (<code>config.py</code>)","text":"<p>Configuration is managed by Pydantic's <code>BaseSettings</code>. This allows for a robust and type-safe configuration system that can read settings from environment variables and <code>.env</code> files. The <code>get_settings</code> function provides a cached instance of the settings, ensuring they are loaded only once.</p> <p>Key features of the configuration system: - Validation: Strong validation of configuration values with custom validators - Type Safety: Full type hints for all configuration options - Environment-based: Support for different environments (development, staging, production) - Computed Properties: Derived configuration values based on other settings - Model Filtering: Advanced model filtering with pattern matching support</p>"},{"location":"architecture/#3-api-endpoints-apipy","title":"3. API Endpoints (<code>api.py</code>)","text":"<p>All API logic is contained within <code>api.py</code>. This module defines the API routes that mimic the Ollama API. Each route handler is responsible for:</p> <ul> <li>Receiving the incoming request.</li> <li>Validating the request body using Pydantic models defined in <code>models.py</code>.</li> <li>Calling the OpenRouter client to perform the requested action.</li> <li>Translating the response from OpenRouter back into the Ollama format.</li> <li>Handling any errors that may occur during the process.</li> </ul> <p>Key features of the API implementation: - Dependency Injection: Uses FastAPI's dependency injection for clean, testable code - Enhanced Error Handling: Context managers for consistent error handling - Streaming Support: Full support for streaming responses from OpenRouter - Model Resolution: Intelligent model name resolution and validation - Request Tracking: Request ID generation and tracking for observability</p>"},{"location":"architecture/#4-openrouter-client-openrouterpy","title":"4. OpenRouter Client (<code>openrouter.py</code>)","text":"<p>This component is responsible for all communication with the OpenRouter API. It uses the <code>httpx</code> library for making asynchronous HTTP requests, which is essential for the proxy's non-blocking, high-performance design.</p> <p>The <code>OpenRouterClient</code> class encapsulates the logic for making requests to OpenRouter's chat, generation, and embedding endpoints.</p> <p>Key features of the OpenRouter client: - Asynchronous HTTP: Uses <code>httpx.AsyncClient</code> for high-performance async requests - Connection Pooling: Efficient connection reuse for better performance - Timeout Management: Configurable timeouts for different types of requests - Error Handling: Comprehensive error handling for various HTTP status codes - Streaming Support: Support for both streaming and non-streaming responses</p>"},{"location":"architecture/#5-data-models-modelspy","title":"5. Data Models (<code>models.py</code>)","text":"<p>This file contains all the Pydantic models that define the data structures for API requests and responses. These models are used for data validation, serialization, and ensuring type safety throughout the application. They are crucial for maintaining compatibility with the Ollama API.</p> <p>Key features of the data models: - Comprehensive Coverage: Models for all supported Ollama API endpoints - Validation: Strict validation of request and response data - Type Safety: Full type hints for all model fields - Extensibility: Easy to extend with new fields or models as needed - Documentation: Clear field descriptions for better understanding</p>"},{"location":"architecture/#6-monitoring-and-metrics-monitoringpy","title":"6. Monitoring and Metrics (<code>monitoring.py</code>)","text":"<p>The monitoring system provides insights into the proxy's performance and health.</p> <p>Key features of the monitoring system: - Metrics Collection: Collects key metrics like request counts, response times, and error rates - Health Checks: Comprehensive health status reporting - Endpoint Statistics: Detailed statistics for each API endpoint - Performance Tracking: Tracks performance metrics over time - Export-ready: Metrics formatted for easy integration with monitoring systems</p>"},{"location":"architecture/#design-principles","title":"Design Principles","text":""},{"location":"architecture/#dependency-injection","title":"Dependency Injection","text":"<p>The application makes extensive use of FastAPI's dependency injection system. This promotes clean, decoupled code. For example, the <code>OpenRouterClient</code> and <code>AppState</code> are provided to the route handlers as dependencies, making the code easier to test and maintain.</p> <p>Benefits of dependency injection: - Testability: Easy to mock dependencies in unit tests - Reusability: Components can be reused in different contexts - Maintainability: Changes to dependencies don't require changes to dependent code - Flexibility: Easy to swap implementations (e.g., different HTTP clients)</p>"},{"location":"architecture/#structured-logging","title":"Structured Logging","text":"<p>The proxy uses the <code>structlog</code> library to produce structured logs in JSON format. This is invaluable for observability in a production environment, as it allows for easy parsing, searching, and filtering of logs. Contextual information, such as request IDs, is automatically added to the logs.</p> <p>Benefits of structured logging: - Searchability: Easy to search and filter logs using tools like ELK stack - Analysis: Structured data can be easily analyzed for trends and patterns - Correlation: Request IDs allow correlating log entries across different components - Automation: Structured logs can be easily processed by automated systems</p>"},{"location":"architecture/#robust-error-handling","title":"Robust Error Handling","text":"<p>A custom exception hierarchy is defined in <code>exceptions.py</code>. This allows for a clear and consistent way of handling errors. A custom middleware catches these exceptions and converts them into the appropriate HTTP responses, ensuring that the client always receives a meaningful error message.</p> <p>Error handling features: - Hierarchical Exceptions: Clear exception hierarchy for different error types - Context Preservation: Error context is preserved for better debugging - Consistent Responses: All errors result in consistent JSON responses - Error Classification: Errors are classified by type for easier handling</p>"},{"location":"architecture/#asynchronous-operations","title":"Asynchronous Operations","text":"<p>To handle a high number of concurrent requests efficiently, the entire request-response cycle is asynchronous. The use of <code>async/await</code> with <code>httpx</code> and FastAPI ensures that the server is non-blocking and can handle I/O-bound operations (like making requests to OpenRouter) without getting blocked.</p> <p>Benefits of asynchronous operations: - High Concurrency: Can handle many concurrent requests with minimal resources - Better Resource Utilization: More efficient use of CPU and memory - Scalability: Scales better under high load - Performance: Lower latency for I/O-bound operations</p>"},{"location":"architecture/#state-management","title":"State Management","text":"<p>The application uses FastAPI's state management to store shared resources like the OpenRouter client and model mappings. This ensures that resources are properly initialized and shared across requests.</p> <p>State management features: - Application Lifecycle: Proper initialization and cleanup of resources - Thread Safety: Safe access to shared resources in concurrent environments - Resource Sharing: Efficient sharing of expensive resources across requests</p>"},{"location":"architecture/#monitoring-and-observability","title":"Monitoring and Observability","text":"<p>The application is designed with monitoring in mind:</p> <ul> <li>Structured Logs: As mentioned, logs are in JSON format for easy analysis.</li> <li>Health Check: The <code>/health</code> endpoint provides a simple way to check the status of the proxy.</li> <li>Metrics: The <code>/metrics</code> endpoint exposes key metrics, such as request counts, error rates, and response times. This can be integrated with monitoring tools like Prometheus to provide a real-time view of the application's performance.</li> </ul>"},{"location":"architecture/#metrics-collection","title":"Metrics Collection","text":"<p>The proxy collects various metrics to help monitor its performance:</p> <ol> <li>Request Metrics:</li> <li>Total request count</li> <li>Request rate</li> <li>Response time distribution</li> <li> <p>Error rates by type</p> </li> <li> <p>Endpoint Metrics:</p> </li> <li>Per-endpoint request counts</li> <li>Per-endpoint response times</li> <li> <p>Per-endpoint error rates</p> </li> <li> <p>System Metrics:</p> </li> <li>Memory usage</li> <li>CPU usage</li> <li>Uptime</li> </ol>"},{"location":"architecture/#health-monitoring","title":"Health Monitoring","text":"<p>The health check system provides comprehensive information about the proxy's status:</p> <ul> <li>Application Health: Overall health status (healthy, degraded, unhealthy)</li> <li>Uptime: How long the application has been running</li> <li>Model Availability: Number of available models</li> <li>Request Statistics: Request and error counts</li> <li>Performance Metrics: Error rates and response times</li> </ul>"},{"location":"architecture/#integration-with-monitoring-systems","title":"Integration with Monitoring Systems","text":"<p>The proxy's metrics endpoint is designed to integrate easily with popular monitoring systems:</p> <ul> <li>Prometheus: Metrics are exposed in a Prometheus-compatible format</li> <li>Datadog: Structured logs can be easily shipped to Datadog</li> <li>CloudWatch: AWS CloudWatch integration for cloud deployments</li> <li>ELK Stack: Elasticsearch, Logstash, and Kibana integration for on-premises deployments</li> </ul>"},{"location":"architecture/#security-considerations","title":"Security Considerations","text":"<p>The architecture includes several security features:</p> <ul> <li>Input Validation: All inputs are validated using Pydantic models</li> <li>API Key Management: Secure handling of OpenRouter API keys</li> <li>Rate Limiting: Configurable request rate limiting</li> <li>Error Sanitization: Sensitive information is not exposed in error messages</li> <li>Secure Dependencies: Regular updates to dependencies to address security vulnerabilities</li> </ul>"},{"location":"architecture/#performance-optimization","title":"Performance Optimization","text":"<p>Several techniques are used to optimize performance:</p> <ul> <li>Connection Reuse: HTTP connections to OpenRouter are reused for better performance</li> <li>Caching: Model mappings and other data are cached to reduce processing time</li> <li>Asynchronous Processing: Non-blocking I/O operations for better throughput</li> <li>Memory Efficiency: Efficient data structures and memory management</li> <li>Resource Pooling: Shared resources like HTTP clients are pooled for reuse</li> </ul>"},{"location":"configuration/","title":"Configuration Guide","text":"<p>The Ollama Proxy is highly configurable, allowing you to tailor its behavior to your specific needs. You can configure the application through environment variables, a <code>.env</code> file, or command-line arguments.</p>"},{"location":"configuration/#configuration-methods","title":"Configuration Methods","text":""},{"location":"configuration/#1-environment-variables","title":"1. Environment Variables","text":"<p>You can set configuration options using environment variables. For example:</p> <pre><code>export OPENROUTER_API_KEY=\"your_api_key\"\nexport PORT=8080\nollama-proxy\n</code></pre>"},{"location":"configuration/#2-env-file","title":"2. <code>.env</code> File","text":"<p>Create a <code>.env</code> file in the project's root directory to store your configuration. This is the recommended approach for development.</p> <pre><code># .env\nOPENROUTER_API_KEY=\"your_openrouter_api_key_here\"\nHOST=0.0.0.0\nPORT=11434\nLOG_LEVEL=INFO\n</code></pre>"},{"location":"configuration/#3-command-line-arguments","title":"3. Command-Line Arguments","text":"<p>You can override settings from the environment or <code>.env</code> file using command-line arguments when you start the server.</p> <pre><code>ollama-proxy --port 8080 --log-level DEBUG\n</code></pre> <p>Priority Order: Command-line arguments &gt; Environment variables &gt; <code>.env</code> file.</p>"},{"location":"configuration/#all-configuration-options","title":"All Configuration Options","text":""},{"location":"configuration/#environment-variables","title":"Environment Variables","text":"Variable Default Description <code>OPENROUTER_API_KEY</code> Required Your API key for OpenRouter. This is essential for the proxy to function. <code>HOST</code> <code>0.0.0.0</code> The host address the server will bind to. Use <code>0.0.0.0</code> to accept connections from any IP. <code>PORT</code> <code>11434</code> The port the server will listen on. <code>11434</code> is the default Ollama port. <code>LOG_LEVEL</code> <code>INFO</code> The logging level. Options are <code>DEBUG</code>, <code>INFO</code>, <code>WARNING</code>, <code>ERROR</code>, <code>CRITICAL</code>. <code>MODELS_FILTER_PATH</code> <code>models-filter.txt</code> The path to the file used for filtering models. See Model Filtering for more details. <code>OPENROUTER_BASE_URL</code> <code>https://openrouter.ai/api/v1</code> The base URL for the OpenRouter API. You typically won't need to change this. <code>OPENROUTER_TIMEOUT</code> <code>300</code> The timeout in seconds for requests made to the OpenRouter API. <code>MAX_CONCURRENT_REQUESTS</code> <code>100</code> The maximum number of concurrent requests the proxy will handle. <code>DEBUG</code> <code>false</code> Set to <code>true</code> to enable debug mode, which provides more verbose logging. <code>RELOAD</code> <code>false</code> Set to <code>true</code> to enable auto-reloading for development. The server will restart when code changes are detected."},{"location":"configuration/#command-line-options","title":"Command-Line Options","text":"<p>To see all available command-line options, run:</p> <pre><code>ollama-proxy --help\n</code></pre> <ul> <li><code>--host</code>: Overrides the <code>HOST</code> environment variable.</li> <li><code>--port</code>: Overrides the <code>PORT</code> environment variable.</li> <li><code>--api-key</code>: Overrides the <code>OPENROUTER_API_KEY</code> environment variable.</li> <li><code>--models-filter</code>: Overrides the <code>MODELS_FILTER_PATH</code> environment variable.</li> <li><code>--log-level</code>: Overrides the <code>LOG_LEVEL</code> environment variable.</li> <li><code>--reload</code>: Overrides the <code>RELOAD</code> environment variable.</li> </ul>"},{"location":"configuration/#model-filtering","title":"Model Filtering","text":"<p>You can control which OpenRouter models are available through the proxy by creating a model filter file. By default, the proxy looks for a file named <code>models-filter.txt</code> in the root directory.</p> <p>To use model filtering:</p> <ol> <li>Create a file (e.g., <code>models-filter.txt</code>).</li> <li>Add the desired model names to the file, one per line. You should use the Ollama-compatible model name (e.g., <code>google/gemini-pro:latest</code>).</li> </ol>"},{"location":"configuration/#basic-model-filtering","title":"Basic Model Filtering","text":"<p>Example <code>models-filter.txt</code>:</p> <pre><code># This is a comment and will be ignored\ngpt-4:latest\nclaude-3-5-sonnet:latest\nllama-2-7b-chat:latest\ngemini-pro:latest\n</code></pre>"},{"location":"configuration/#advanced-model-filtering","title":"Advanced Model Filtering","text":"<p>The model filter supports more advanced patterns:</p> <pre><code># Exact model names\ngpt-4:latest\nclaude-3-5-sonnet:latest\n\n# Wildcard patterns\nllama*:latest\n*gemini*:*\nclaude*:*\n\n# Exclude specific models (prefix with !)\n!llama-2-13b-chat:latest\n!gpt-3-5-turbo:*\n\n# Comments for organization\n# OpenAI models\ngpt-4:latest\ngpt-3-5-turbo:latest\n\n# Anthropic models\nclaude-3-5-sonnet:latest\nclaude-3-haiku:latest\n</code></pre>"},{"location":"configuration/#filter-file-locations","title":"Filter File Locations","text":"<p>You can specify a custom filter file location:</p> <pre><code># Using environment variable\nexport MODELS_FILTER_PATH=\"/path/to/your/custom-filter.txt\"\nollama-proxy\n\n# Using command-line argument\nollama-proxy --models-filter /path/to/your/custom-filter.txt\n</code></pre>"},{"location":"configuration/#testing-your-filter","title":"Testing Your Filter","text":"<p>To test your filter configuration:</p> <ol> <li> <p>Start the proxy with your filter:    <pre><code>ollama-proxy --models-filter ./your-filter.txt\n</code></pre></p> </li> <li> <p>Check which models are available:    <pre><code>curl http://localhost:11434/api/tags\n</code></pre></p> </li> <li> <p>Or use the Ollama CLI:    <pre><code>ollama list\n</code></pre></p> </li> </ol> <p>If the filter file is empty or does not exist, all models from OpenRouter will be available.</p>"},{"location":"configuration/#environment-specific-configuration","title":"Environment-Specific Configuration","text":""},{"location":"configuration/#development-configuration","title":"Development Configuration","text":"<p>For development, you might want more verbose logging and auto-reload:</p> <pre><code># .env.development\nOPENROUTER_API_KEY=\"your_dev_api_key\"\nHOST=localhost\nPORT=11434\nLOG_LEVEL=DEBUG\nRELOAD=true\nMODELS_FILTER_PATH=./dev-models-filter.txt\n</code></pre>"},{"location":"configuration/#production-configuration","title":"Production Configuration","text":"<p>For production, you might want stricter settings:</p> <pre><code># .env.production\nOPENROUTER_API_KEY=\"your_production_api_key\"\nHOST=0.0.0.0\nPORT=11434\nLOG_LEVEL=WARNING\nMAX_CONCURRENT_REQUESTS=200\nMODELS_FILTER_PATH=./prod-models-filter.txt\n</code></pre>"},{"location":"configuration/#configuration-validation","title":"Configuration Validation","text":"<p>The proxy validates configuration at startup. If there are any issues, you'll see error messages indicating what needs to be fixed:</p> <pre><code># Example error for missing API key\nError loading configuration: 1 validation error for Settings\nopenrouter_api_key\n  Field required [type=missing, input_value={}, input_type=dict]\n</code></pre> <p>To verify your configuration is correct before starting the server:</p> <pre><code># Test with a dry run (if supported)\nollama-proxy --dry-run\n\n# Or just check the help\nollama-proxy --help\n</code></pre>"},{"location":"contributing/","title":"Contributing to Ollama Proxy","text":"<p>We welcome contributions from the community! Whether you're fixing a bug, adding a new feature, or improving the documentation, your help is greatly appreciated.</p>"},{"location":"contributing/#how-to-contribute","title":"How to Contribute","text":""},{"location":"contributing/#getting-started","title":"Getting Started","text":"<ol> <li>Fork the repository on GitHub.</li> <li>Clone your fork locally:     <pre><code>git clone https://github.com/your-username/ollama-proxy.git\ncd ollama-proxy\n</code></pre></li> <li>Set up a virtual environment and install the dependencies, including the test dependencies:     <pre><code>python -m venv .venv\nsource .venv/bin/activate\npip install -e \".[test]\"\n</code></pre></li> </ol>"},{"location":"contributing/#making-changes","title":"Making Changes","text":"<ol> <li>Create a new branch for your feature or bug fix:     <pre><code>git checkout -b my-new-feature\n</code></pre></li> <li>Make your changes to the code or documentation.</li> <li>Write tests for any new features or bug fixes. We use <code>pytest</code> for testing.</li> <li>Run the tests to ensure that everything is working correctly:     <pre><code>pytest\n</code></pre></li> <li>Format your code using a code formatter if necessary. This project uses <code>black</code> and <code>isort</code>.</li> </ol>"},{"location":"contributing/#submitting-your-contribution","title":"Submitting Your Contribution","text":"<ol> <li>Commit your changes with a clear and descriptive commit message:     <pre><code>git commit -m \"feat: Add support for a new API endpoint\"\n</code></pre></li> <li>Push your branch to your fork on GitHub:     <pre><code>git push origin my-new-feature\n</code></pre></li> <li>Open a pull request from your branch to the <code>main</code> branch of the original repository.</li> <li>Provide a detailed description of your changes in the pull request.</li> </ol>"},{"location":"contributing/#coding-guidelines","title":"Coding Guidelines","text":"<ul> <li>Follow the existing code style.</li> <li>Write clear and concise comments where necessary.</li> <li>Ensure your code is well-tested.</li> <li>Update the documentation if you are adding or changing a feature.</li> </ul>"},{"location":"contributing/#reporting-bugs","title":"Reporting Bugs","text":"<p>If you find a bug, please open an issue on the GitHub repository. Include the following information in your report:</p> <ul> <li>A clear and descriptive title.</li> <li>A detailed description of the bug.</li> <li>Steps to reproduce the bug.</li> <li>Any relevant logs or error messages.</li> </ul> <p>Thank you for contributing to the Ollama Proxy project!</p>"},{"location":"deployment/","title":"Deployment Guide","text":"<p>This guide provides instructions for deploying the Ollama Proxy in a production environment. The recommended deployment method is using Docker, but we also cover other deployment options.</p>"},{"location":"deployment/#docker-deployment-recommended","title":"Docker Deployment (Recommended)","text":"<p>Using Docker is the most reliable way to run the Ollama Proxy, as it encapsulates the application and its dependencies in a container.</p>"},{"location":"deployment/#prerequisites","title":"Prerequisites","text":"<ul> <li>Docker installed on your server.</li> <li>Docker Compose (optional, but recommended for easier management).</li> </ul>"},{"location":"deployment/#method-1-using-docker-composeyml","title":"Method 1: Using <code>docker-compose.yml</code>","text":"<p>The project includes a <code>docker-compose.yml</code> file to simplify deployment.</p> <ol> <li> <p>Create an environment file:     Create a file named <code>.env.production</code> in the project root and add your production-ready settings:</p> <pre><code># .env.production\nOPENROUTER_API_KEY=\"your_production_api_key\"\nLOG_LEVEL=INFO\nMAX_CONCURRENT_REQUESTS=200\n</code></pre> </li> <li> <p>Start the server:     Run the following command to build and start the container in detached mode:</p> <pre><code>docker-compose up --build -d\n</code></pre> </li> <li> <p>Verify the deployment:     Check the logs to ensure the server started correctly:</p> <pre><code>docker-compose logs -f\n</code></pre> <p>You should see output indicating that the Uvicorn server is running.</p> </li> </ol>"},{"location":"deployment/#method-2-using-dockerfile","title":"Method 2: Using <code>Dockerfile</code>","text":"<p>If you prefer not to use Docker Compose, you can build and run the Docker image manually.</p> <ol> <li> <p>Build the Docker image:     From the project root, run:</p> <pre><code>docker build -t ollama-proxy .\n</code></pre> </li> <li> <p>Run the Docker container:     Be sure to pass in your OpenRouter API key and expose the port.</p> <pre><code>docker run -d -p 11434:11434 \\\n  -e OPENROUTER_API_KEY=\"your_production_api_key\" \\\n  --name ollama-proxy-container \\\n  ollama-proxy\n</code></pre> </li> </ol>"},{"location":"deployment/#advanced-docker-configuration","title":"Advanced Docker Configuration","text":"<p>For production deployments, you might want to use volume mounts for configuration files:</p> <pre><code>docker run -d -p 11434:11434 \\\n  -e OPENROUTER_API_KEY=\"your_production_api_key\" \\\n  -v /path/to/your/models-filter.txt:/app/models-filter.txt \\\n  -v /path/to/your/logs:/app/logs \\\n  --name ollama-proxy-container \\\n  ollama-proxy\n</code></pre>"},{"location":"deployment/#systemd-deployment-linux","title":"Systemd Deployment (Linux)","text":"<p>For direct deployment on a Linux system, you can use systemd to manage the service.</p> <ol> <li> <p>Create a service file:     Create <code>/etc/systemd/system/ollama-proxy.service</code>:</p> <pre><code>[Unit]\nDescription=Ollama Proxy Service\nAfter=network.target\n\n[Service]\nType=simple\nUser=ollama-proxy\nWorkingDirectory=/opt/ollama-proxy\nEnvironment=OPENROUTER_API_KEY=your_production_api_key\nEnvironment=LOG_LEVEL=INFO\nExecStart=/opt/ollama-proxy/.venv/bin/ollama-proxy\nRestart=always\nRestartSec=5\n\n[Install]\nWantedBy=multi-user.target\n</code></pre> </li> <li> <p>Create a dedicated user:     <pre><code>sudo useradd -r -s /bin/false ollama-proxy\n</code></pre></p> </li> <li> <p>Set up the application directory:     <pre><code>sudo mkdir -p /opt/ollama-proxy\nsudo chown ollama-proxy:ollama-proxy /opt/ollama-proxy\n</code></pre></p> </li> <li> <p>Install dependencies and set up the virtual environment:     <pre><code>sudo -u ollama-proxy bash -c \"\n  cd /opt/ollama-proxy\n  python -m venv .venv\n  source .venv/bin/activate\n  pip install -e .\n\"\n</code></pre></p> </li> <li> <p>Enable and start the service:     <pre><code>sudo systemctl enable ollama-proxy\nsudo systemctl start ollama-proxy\n</code></pre></p> </li> <li> <p>Check the service status:     <pre><code>sudo systemctl status ollama-proxy\n</code></pre></p> </li> </ol>"},{"location":"deployment/#kubernetes-deployment","title":"Kubernetes Deployment","text":"<p>For Kubernetes deployments, you can use a deployment manifest like this:</p> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: ollama-proxy\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: ollama-proxy\n  template:\n    metadata:\n      labels:\n        app: ollama-proxy\n    spec:\n      containers:\n      - name: ollama-proxy\n        image: your-registry/ollama-proxy:latest\n        ports:\n        - containerPort: 11434\n        env:\n        - name: OPENROUTER_API_KEY\n          valueFrom:\n            secretKeyRef:\n              name: ollama-proxy-secrets\n              key: api-key\n        - name: LOG_LEVEL\n          value: \"INFO\"\n        resources:\n          requests:\n            memory: \"256Mi\"\n            cpu: \"250m\"\n          limits:\n            memory: \"512Mi\"\n            cpu: \"500m\"\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: ollama-proxy-service\nspec:\n  selector:\n    app: ollama-proxy\n  ports:\n  - protocol: TCP\n    port: 11434\n    targetPort: 11434\n  type: LoadBalancer\n</code></pre> <p>And create the secret for your API key:</p> <pre><code>kubectl create secret generic ollama-proxy-secrets \\\n  --from-literal=api-key='your_production_api_key'\n</code></pre>"},{"location":"deployment/#production-best-practices","title":"Production Best Practices","text":""},{"location":"deployment/#security","title":"Security","text":"<ul> <li>Secure your API Key: Never hardcode your OpenRouter API key in scripts or configuration files that might be committed to version control. Use environment variables, secrets management tools, or Kubernetes secrets.</li> <li>Network Security: Restrict access to the proxy using firewall rules or network policies. Only expose the necessary ports.</li> <li>Use HTTPS: In production, always use HTTPS. You can terminate TLS at a reverse proxy like nginx or use a service that provides automatic TLS termination.</li> </ul>"},{"location":"deployment/#performance","title":"Performance","text":"<ul> <li>Resource Limits: Set appropriate CPU and memory limits for your containers or processes.</li> <li>Concurrency: Adjust <code>MAX_CONCURRENT_REQUESTS</code> based on your expected load and available resources.</li> <li>Model Filtering: Use model filtering to limit the number of models available, which can reduce memory usage and improve startup time.</li> </ul>"},{"location":"deployment/#monitoring-and-observability","title":"Monitoring and Observability","text":"<ul> <li>Health Checks: Use the <code>/health</code> endpoint for load balancer health checks.</li> <li>Metrics: The proxy includes a <code>/metrics</code> endpoint that can be integrated with monitoring systems like Prometheus to track performance and errors. For more details, see the Architecture Guide.</li> <li>Logging: In a production environment, configure the <code>LOG_LEVEL</code> to <code>INFO</code> or <code>WARNING</code> and ship your logs to a centralized logging platform like ELK stack, Datadog, or CloudWatch for analysis and alerting.</li> <li>Alerting: Set up alerts for high error rates, slow response times, or service downtime.</li> </ul>"},{"location":"deployment/#backup-and-recovery","title":"Backup and Recovery","text":"<ul> <li>Configuration Backups: Keep backups of your configuration files and model filter files.</li> <li>Regular Updates: Regularly update the proxy to get the latest features and security fixes.</li> <li>Rolling Updates: When using orchestration platforms like Kubernetes, use rolling updates to minimize downtime during deployments.</li> </ul>"},{"location":"deployment/#scaling","title":"Scaling","text":"<ul> <li>Horizontal Scaling: The proxy is stateless, so you can run multiple instances behind a load balancer to handle more traffic.</li> <li>Load Balancing: Use a load balancer to distribute traffic across multiple proxy instances.</li> <li>Auto-scaling: Configure auto-scaling policies based on CPU or memory usage, or request rate.</li> </ul>"},{"location":"troubleshooting/","title":"Troubleshooting Guide","text":"<p>This guide provides solutions to common problems you might encounter while using the Ollama Proxy. It includes diagnostic steps and solutions for various issues.</p>"},{"location":"troubleshooting/#connection-issues","title":"Connection Issues","text":""},{"location":"troubleshooting/#connection-refused-error","title":"<code>Connection refused</code> error","text":"<p>If you get a <code>Connection refused</code> error when trying to connect to the proxy, it could be due to several reasons:</p> <p>Diagnostic Steps:</p> <ol> <li> <p>Check if the proxy is running:    <pre><code># If running with Docker\ndocker ps | grep ollama-proxy\n\n# Check Docker logs\ndocker logs ollama-proxy-container\n\n# If running directly, check process\nps aux | grep ollama-proxy\n</code></pre></p> </li> <li> <p>Verify the port is listening:    <pre><code># Check if the port is open\nnetstat -tlnp | grep :11434\n\n# Or using lsof (if available)\nlsof -i :11434\n\n# Or using ss (if netstat is not available)\nss -tlnp | grep :11434\n</code></pre></p> </li> <li> <p>Test local connectivity:    <pre><code># Test basic connectivity\ncurl -v http://localhost:11434/\n\n# Test API endpoint\ncurl -v http://localhost:11434/api/version\n</code></pre></p> </li> </ol> <p>Solutions:</p> <ul> <li>The proxy is not running: Make sure you have started the proxy server. You can check the logs to confirm it is running.</li> <li>Incorrect host or port: Ensure that your client is configured to connect to the correct host and port. The default is <code>http://localhost:11434</code>.</li> <li>Firewall issues: A firewall on your system or network might be blocking the connection. Check your firewall settings to ensure that the port is open.</li> <li>Docker networking issues: If using Docker, ensure the container is properly exposing the port:     <pre><code># Check port mapping\ndocker port ollama-proxy-container\n\n# Run with explicit port mapping\ndocker run -d -p 11434:11434 \\\n  -e OPENROUTER_API_KEY=\"your_api_key\" \\\n  --name ollama-proxy-container \\\n  ollama-proxy\n</code></pre></li> </ul>"},{"location":"troubleshooting/#connection-timeout-error","title":"<code>Connection timeout</code> error","text":"<p>This error typically indicates network connectivity issues.</p> <p>Diagnostic Steps:</p> <ol> <li> <p>Test internet connectivity:    <pre><code>ping openrouter.ai\n</code></pre></p> </li> <li> <p>Test DNS resolution:    <pre><code>nslookup openrouter.ai\ndig openrouter.ai\n</code></pre></p> </li> <li> <p>Test direct OpenRouter connectivity:    <pre><code>curl -H \"Authorization: Bearer $OPENROUTER_API_KEY\" \\\n  https://openrouter.ai/api/v1/models\n</code></pre></p> </li> </ol> <p>Solutions:</p> <ul> <li>Network connectivity: Ensure your system has internet access.</li> <li>DNS issues: Try using a different DNS server (e.g., Google's 8.8.8.8).</li> <li>Proxy configuration: If you're behind a corporate proxy, configure the proxy settings appropriately.</li> </ul>"},{"location":"troubleshooting/#model-errors","title":"Model Errors","text":""},{"location":"troubleshooting/#model-not-found-error","title":"<code>Model not found</code> error","text":"<p>If you receive a <code>Model not found</code> error, it could be because:</p> <p>Diagnostic Steps:</p> <ol> <li> <p>List available models:    <pre><code># Using curl\ncurl http://localhost:11434/api/tags\n\n# Using Ollama CLI\nollama list\n</code></pre></p> </li> <li> <p>Check OpenRouter directly:    <pre><code>curl -H \"Authorization: Bearer $OPENROUTER_API_KEY\" \\\n  https://openrouter.ai/api/v1/models\n</code></pre></p> </li> <li> <p>Verify model filter configuration:    <pre><code># Check if filter file exists\ncat models-filter.txt\n\n# Check proxy logs for filter information\ndocker logs ollama-proxy-container | grep -i filter\n</code></pre></p> </li> </ol> <p>Solutions:</p> <ul> <li>The model does not exist on OpenRouter: Double-check the model name to ensure it is correct. You can find a list of available models on the OpenRouter website.</li> <li>The model is not spelled correctly: Model names are case-sensitive. Use the exact model name as shown in the <code>/api/tags</code> response.</li> <li>The model is filtered out: If you are using a model filter file, make sure the model you are trying to use is included in the file. See the Configuration Guide for more details.</li> </ul>"},{"location":"troubleshooting/#model-is-not-allowed-error","title":"<code>Model is not allowed</code> error","text":"<p>This error means that the model you are trying to use has been blocked by the model filter.</p> <p>Diagnostic Steps:</p> <ol> <li> <p>Check your filter file:    <pre><code>cat models-filter.txt\n</code></pre></p> </li> <li> <p>Check proxy logs:    <pre><code>docker logs ollama-proxy-container | grep -i \"not allowed\"\n</code></pre></p> </li> </ol> <p>Solutions:</p> <ul> <li>Add the model to your filter: Add the model to your <code>models-filter.txt</code> file.</li> <li>Temporarily disable filtering: Start the proxy with an empty filter to test:    <pre><code>ollama-proxy --models-filter \"\"\n</code></pre></li> <li>Check filter syntax: Ensure your filter file syntax is correct with no typos.</li> </ul>"},{"location":"troubleshooting/#api-key-errors","title":"API Key Errors","text":""},{"location":"troubleshooting/#401-unauthorized-error","title":"<code>401 Unauthorized</code> error","text":"<p>A <code>401 Unauthorized</code> error from the proxy indicates a problem with your OpenRouter API key.</p> <p>Diagnostic Steps:</p> <ol> <li> <p>Verify environment variable:    <pre><code>echo $OPENROUTER_API_KEY\n</code></pre></p> </li> <li> <p>Check .env file:    <pre><code>cat .env\n</code></pre></p> </li> <li> <p>Test API key directly:    <pre><code>curl -H \"Authorization: Bearer $OPENROUTER_API_KEY\" \\\n  https://openrouter.ai/api/v1/models\n</code></pre></p> </li> </ol> <p>Solutions:</p> <ul> <li>The API key is missing: Make sure you have set the <code>OPENROUTER_API_KEY</code> environment variable or added it to your <code>.env</code> file.</li> <li>The API key is invalid: Verify that your API key is correct and has not expired. You can check your API key on the OpenRouter website.</li> <li>Key format issues: Ensure your API key doesn't have extra spaces or quotes:    <pre><code># Correct\nOPENROUTER_API_KEY=sk-youractualkeyhere\n\n# Incorrect - avoid these:\n# OPENROUTER_API_KEY=\"sk-youractualkeyhere\"  # No quotes needed\n# OPENROUTER_API_KEY= sk-youractualkeyhere   # No leading spaces\n</code></pre></li> </ul>"},{"location":"troubleshooting/#403-forbidden-error","title":"<code>403 Forbidden</code> error","text":"<p>This error usually indicates that your API key doesn't have access to a specific model.</p> <p>Solutions:</p> <ul> <li>Check model availability: Some models on OpenRouter require special access or have usage limits.</li> <li>Verify billing: Ensure your OpenRouter account has valid billing information if required for the model.</li> <li>Try a different model: Test with a different model to see if the issue is model-specific.</li> </ul>"},{"location":"troubleshooting/#performance-issues","title":"Performance Issues","text":""},{"location":"troubleshooting/#slow-responses","title":"Slow responses","text":"<p>If you are experiencing slow responses, it could be due to:</p> <p>Diagnostic Steps:</p> <ol> <li> <p>Check proxy logs for timing information:    <pre><code>docker logs ollama-proxy-container | grep -i \"completed\\|duration\"\n</code></pre></p> </li> <li> <p>Test direct OpenRouter response time:    <pre><code>time curl -H \"Authorization: Bearer $OPENROUTER_API_KEY\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"model\": \"google/gemini-pro:latest\", \"messages\": [{\"role\": \"user\", \"content\": \"Hello\"}]}' \\\n  https://openrouter.ai/api/v1/chat/completions\n</code></pre></p> </li> <li> <p>Check system resources:    <pre><code># Check CPU and memory usage\ntop\n\n# Check Docker resource usage\ndocker stats ollama-proxy-container\n</code></pre></p> </li> </ol> <p>Solutions:</p> <ul> <li>High latency to OpenRouter: The proxy needs to make requests to the OpenRouter API, so your connection to OpenRouter can affect performance.</li> <li>Large models: Larger models can take longer to generate responses. Consider using smaller models for faster responses.</li> <li>High server load: If the proxy is handling a large number of concurrent requests, it may slow down. Consider:</li> <li>Increasing <code>MAX_CONCURRENT_REQUESTS</code> if your system can handle more</li> <li>Adding more proxy instances behind a load balancer</li> <li>Upgrading your system resources</li> </ul>"},{"location":"troubleshooting/#high-memory-usage","title":"High memory usage","text":"<p>Diagnostic Steps:</p> <ol> <li> <p>Check Docker container memory usage:    <pre><code>docker stats ollama-proxy-container\n</code></pre></p> </li> <li> <p>Check system memory:    <pre><code>free -h\n</code></pre></p> </li> </ol> <p>Solutions:</p> <ul> <li>Model filtering: Use model filtering to reduce the number of models loaded</li> <li>System resources: Ensure your system has adequate memory</li> <li>Docker limits: Set memory limits for Docker containers:    <pre><code>docker run -d -p 11434:11434 \\\n  -e OPENROUTER_API_KEY=\"your_api_key\" \\\n  --memory=512m \\\n  --name ollama-proxy-container \\\n  ollama-proxy\n</code></pre></li> </ul>"},{"location":"troubleshooting/#logging-and-debugging","title":"Logging and Debugging","text":""},{"location":"troubleshooting/#enabling-debug-logging","title":"Enabling Debug Logging","text":"<p>For more detailed information, enable debug logging:</p> <pre><code># Using environment variable\nexport LOG_LEVEL=DEBUG\nollama-proxy\n\n# Using command line\nollama-proxy --log-level DEBUG\n\n# In Docker\ndocker run -d -p 11434:11434 \\\n  -e OPENROUTER_API_KEY=\"your_api_key\" \\\n  -e LOG_LEVEL=DEBUG \\\n  --name ollama-proxy-container \\\n  ollama-proxy\n</code></pre>"},{"location":"troubleshooting/#checking-health-status","title":"Checking Health Status","text":"<p>Use the health endpoint to get detailed status information:</p> <pre><code>curl http://localhost:11434/health\n</code></pre>"},{"location":"troubleshooting/#monitoring-metrics","title":"Monitoring Metrics","text":"<p>Check the metrics endpoint for performance data:</p> <pre><code>curl http://localhost:11434/metrics\n</code></pre>"},{"location":"troubleshooting/#docker-specific-issues","title":"Docker-Specific Issues","text":""},{"location":"troubleshooting/#container-wont-start","title":"Container won't start","text":"<p>Diagnostic Steps:</p> <ol> <li> <p>Check container status:    <pre><code>docker ps -a\n</code></pre></p> </li> <li> <p>Check container logs:    <pre><code>docker logs ollama-proxy-container\n</code></pre></p> </li> </ol> <p>Solutions:</p> <ul> <li>Configuration errors: Check logs for configuration validation errors</li> <li>Port conflicts: Ensure the port is not already in use:    <pre><code>docker run -d -p 8080:11434 \\  # Use different host port\n  -e OPENROUTER_API_KEY=\"your_api_key\" \\\n  --name ollama-proxy-container \\\n  ollama-proxy\n</code></pre></li> </ul>"},{"location":"troubleshooting/#volume-mounting-issues","title":"Volume mounting issues","text":"<p>When using volume mounts, ensure proper permissions:</p> <pre><code># Create directories with correct permissions\nmkdir -p /host/path/to/config\nchmod 755 /host/path/to/config\n\n# Run with volume mount\ndocker run -d -p 11434:11434 \\\n  -e OPENROUTER_API_KEY=\"your_api_key\" \\\n  -v /host/path/to/config:/app/config \\\n  --name ollama-proxy-container \\\n  ollama-proxy\n</code></pre>"},{"location":"troubleshooting/#advanced-debugging","title":"Advanced Debugging","text":""},{"location":"troubleshooting/#network-debugging","title":"Network debugging","text":"<p>If you suspect network issues between the proxy and OpenRouter:</p> <ol> <li> <p>Test connectivity from within the container:    <pre><code>docker exec -it ollama-proxy-container sh\nping openrouter.ai\ncurl https://openrouter.ai/api/v1/models\n</code></pre></p> </li> <li> <p>Check DNS resolution:    <pre><code>docker exec -it ollama-proxy-container cat /etc/resolv.conf\n</code></pre></p> </li> </ol>"},{"location":"troubleshooting/#testing-with-different-clients","title":"Testing with different clients","text":"<p>Try different clients to isolate issues:</p> <ol> <li> <p>Direct curl requests:    <pre><code>curl http://localhost:11434/api/chat \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"google/gemini-pro:latest\",\n    \"messages\": [\n      {\"role\": \"user\", \"content\": \"Hello\"}\n    ]\n  }'\n</code></pre></p> </li> <li> <p>Ollama CLI:    <pre><code>ollama run gemini-pro:latest \"Hello\"\n</code></pre></p> </li> </ol>"},{"location":"troubleshooting/#getting-help","title":"Getting Help","text":"<p>If you are still having trouble after consulting this guide:</p> <ol> <li>Gather diagnostic information:</li> <li>Proxy logs</li> <li>Output of <code>docker ps</code> and <code>docker logs</code></li> <li>Your configuration files (with sensitive information redacted)</li> <li> <p>Steps to reproduce the issue</p> </li> <li> <p>Open an issue: Please open an issue on GitHub with a detailed description of the problem, including:</p> </li> <li>Your environment (OS, Docker version, etc.)</li> <li>Steps to reproduce</li> <li>Expected vs. actual behavior</li> <li> <p>Any relevant logs or error messages</p> </li> <li> <p>Community support: Check if similar issues have been discussed in existing issues or community forums.</p> </li> </ol>"},{"location":"usage-examples/","title":"Usage Examples","text":"<p>This guide provides practical examples of how to use the Ollama Proxy with various tools and applications. These examples will help you get started quickly and show you how to integrate the proxy into your existing workflows.</p>"},{"location":"usage-examples/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Basic Usage with Ollama CLI</li> <li>Using with Ollama Clients</li> <li>Programming Language Examples</li> <li>Advanced Configuration Examples</li> <li>Troubleshooting Common Issues</li> </ul>"},{"location":"usage-examples/#basic-usage-with-ollama-cli","title":"Basic Usage with Ollama CLI","text":""},{"location":"usage-examples/#listing-available-models","title":"Listing Available Models","text":"<p>Once the proxy is running, you can list all available models:</p> <pre><code># List models through the proxy\nollama list\n\n# Or using curl directly\ncurl http://localhost:11434/api/tags\n</code></pre>"},{"location":"usage-examples/#chat-with-a-model","title":"Chat with a Model","text":"<p>You can chat with any model available through OpenRouter:</p> <pre><code># Start a chat session\nollama run gemini-pro:latest\n\n# Or send a single message\necho \"Why is the sky blue?\" | ollama run gemini-pro:latest\n</code></pre>"},{"location":"usage-examples/#generate-text","title":"Generate Text","text":"<p>Generate text using a model:</p> <pre><code># Simple text generation\nollama run gemini-pro:latest \"Write a short poem about programming\"\n\n# Using prompt files\nollama run gemini-pro:latest -f ./prompt.txt\n</code></pre>"},{"location":"usage-examples/#using-with-ollama-clients","title":"Using with Ollama Clients","text":""},{"location":"usage-examples/#python-client-example","title":"Python Client Example","text":"<p>If you're using the Ollama Python client, you can configure it to use the proxy:</p> <pre><code>import ollama\n\n# Configure client to use proxy\nclient = ollama.Client(host='http://localhost:11434')\n\n# List models\nmodels = client.list()\nprint(models)\n\n# Chat with a model\nresponse = client.chat(\n    model='gemini-pro:latest',\n    messages=[{\n        'role': 'user',\n        'content': 'Why is the sky blue?',\n    }]\n)\nprint(response['message']['content'])\n</code></pre>"},{"location":"usage-examples/#javascriptnodejs-client-example","title":"JavaScript/Node.js Client Example","text":"<p>For the JavaScript client:</p> <pre><code>import ollama from 'ollama';\n\n// The client will automatically use http://localhost:11434\n// unless you specify a different host\n\n// List models\nconst models = await ollama.list();\nconsole.log(models);\n\n// Chat with a model\nconst response = await ollama.chat({\n  model: 'gemini-pro:latest',\n  messages: [{ role: 'user', content: 'Why is the sky blue?' }]\n});\nconsole.log(response.message.content);\n</code></pre>"},{"location":"usage-examples/#programming-language-examples","title":"Programming Language Examples","text":""},{"location":"usage-examples/#python-direct-api-usage","title":"Python Direct API Usage","text":"<p>You can also interact with the proxy directly using HTTP requests:</p> <pre><code>import requests\nimport json\n\n# Proxy endpoint\nproxy_url = 'http://localhost:11434'\n\n# List available models\nresponse = requests.get(f'{proxy_url}/api/tags')\nmodels = response.json()\nprint(\"Available models:\", [m['name'] for m in models['models']])\n\n# Chat completion\nchat_data = {\n    \"model\": \"gemini-pro:latest\",\n    \"messages\": [\n        {\n            \"role\": \"user\",\n            \"content\": \"Explain quantum computing in simple terms\"\n        }\n    ]\n}\n\nresponse = requests.post(\n    f'{proxy_url}/api/chat',\n    headers={'Content-Type': 'application/json'},\n    data=json.dumps(chat_data)\n)\n\nif response.status_code == 200:\n    result = response.json()\n    print(\"Response:\", result['message']['content'])\nelse:\n    print(f\"Error: {response.status_code} - {response.text}\")\n</code></pre>"},{"location":"usage-examples/#streaming-responses","title":"Streaming Responses","text":"<p>To handle streaming responses in Python:</p> <pre><code>import requests\nimport json\n\nproxy_url = 'http://localhost:11434'\n\n# Streaming chat completion\nchat_data = {\n    \"model\": \"gemini-pro:latest\",\n    \"messages\": [\n        {\n            \"role\": \"user\",\n            \"content\": \"Write a story about a robot learning to paint\"\n        }\n    ],\n    \"stream\": True\n}\n\nwith requests.post(\n    f'{proxy_url}/api/chat',\n    headers={'Content-Type': 'application/json'},\n    data=json.dumps(chat_data),\n    stream=True\n) as response:\n    for line in response.iter_lines():\n        if line:\n            chunk = json.loads(line.decode('utf-8'))\n            if 'message' in chunk and 'content' in chunk['message']:\n                print(chunk['message']['content'], end='', flush=True)\n            if chunk.get('done', False):\n                break\n</code></pre>"},{"location":"usage-examples/#curl-examples","title":"cURL Examples","text":"<p>You can also use cURL to interact with the proxy:</p> <pre><code># List models\ncurl http://localhost:11434/api/tags\n\n# Chat completion\ncurl http://localhost:11434/api/chat \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"gemini-pro:latest\",\n    \"messages\": [\n      {\n        \"role\": \"user\",\n        \"content\": \"What is the capital of France?\"\n      }\n    ]\n  }'\n\n# Streaming chat completion\ncurl http://localhost:11434/api/chat \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"gemini-pro:latest\",\n    \"messages\": [\n      {\n        \"role\": \"user\",\n        \"content\": \"Count from 1 to 100.\"\n      }\n    ],\n    \"stream\": true\n  }'\n</code></pre>"},{"location":"usage-examples/#advanced-configuration-examples","title":"Advanced Configuration Examples","text":""},{"location":"usage-examples/#model-filtering","title":"Model Filtering","text":"<p>Create a custom <code>models-filter.txt</code> to limit which models are available:</p> <pre><code># Only allow these specific models\ngemini-pro:latest\ngpt-4:latest\nclaude-3-5-sonnet:latest\n\n# Allow all models from a specific family using wildcards\nllama*:*latest\n\n# Exclude specific models (prefix with !)\n!llama-2-13b-chat:latest\n</code></pre> <p>Then start the proxy with your filter:</p> <pre><code>ollama-proxy --models-filter ./models-filter.txt\n</code></pre>"},{"location":"usage-examples/#environment-based-configuration","title":"Environment-based Configuration","text":"<p>Create a <code>.env</code> file for different environments:</p> <pre><code># Development environment\nOPENROUTER_API_KEY=your_dev_api_key_here\nLOG_LEVEL=DEBUG\nHOST=localhost\nPORT=11434\nMODELS_FILTER_PATH=./dev-models-filter.txt\n</code></pre> <p>For production:</p> <pre><code># Production environment\nOPENROUTER_API_KEY=your_prod_api_key_here\nLOG_LEVEL=WARNING\nHOST=0.0.0.0\nPORT=11434\nMODELS_FILTER_PATH=./prod-models-filter.txt\nMAX_CONCURRENT_REQUESTS=200\n</code></pre>"},{"location":"usage-examples/#docker-compose-with-custom-configuration","title":"Docker Compose with Custom Configuration","text":"<p>Create a <code>docker-compose.prod.yml</code> for production deployment:</p> <pre><code>version: '3.8'\nservices:\n  ollama-proxy:\n    build: .\n    ports:\n      - \"11434:11434\"\n    environment:\n      - OPENROUTER_API_KEY=${OPENROUTER_API_KEY}\n      - LOG_LEVEL=INFO\n      - MAX_CONCURRENT_REQUESTS=200\n    volumes:\n      - ./prod-models-filter.txt:/app/models-filter.txt\n    restart: unless-stopped\n</code></pre>"},{"location":"usage-examples/#troubleshooting-common-issues","title":"Troubleshooting Common Issues","text":""},{"location":"usage-examples/#connection-issues","title":"Connection Issues","text":"<p>If you can't connect to the proxy:</p> <ol> <li> <p>Check if the proxy is running:    <pre><code># Check if the port is listening\nnetstat -tlnp | grep :11434\n\n# Or use lsof\nlsof -i :11434\n</code></pre></p> </li> <li> <p>Verify the proxy logs:    <pre><code># If running with Docker\ndocker logs ollama-proxy-container\n\n# If running directly\n# Check your terminal where you started the proxy\n</code></pre></p> </li> </ol>"},{"location":"usage-examples/#model-not-found-errors","title":"Model Not Found Errors","text":"<p>If you get \"model not found\" errors:</p> <ol> <li> <p>List available models to see what's actually available:    <pre><code>curl http://localhost:11434/api/tags\n</code></pre></p> </li> <li> <p>Check your model filter configuration:    <pre><code># View your filter file\ncat models-filter.txt\n</code></pre></p> </li> <li> <p>Try without filtering to see all models:    <pre><code>ollama-proxy --models-filter \"\"\n</code></pre></p> </li> </ol>"},{"location":"usage-examples/#api-key-issues","title":"API Key Issues","text":"<p>If you get authentication errors:</p> <ol> <li> <p>Verify your API key:    <pre><code>echo $OPENROUTER_API_KEY\n</code></pre></p> </li> <li> <p>Test the key directly with OpenRouter:    <pre><code>curl https://openrouter.ai/api/v1/models \\\n  -H \"Authorization: Bearer $OPENROUTER_API_KEY\"\n</code></pre></p> </li> </ol>"},{"location":"usage-examples/#performance-issues","title":"Performance Issues","text":"<p>If responses are slow:</p> <ol> <li> <p>Check the proxy logs for any errors or warnings.</p> </li> <li> <p>Monitor the proxy's health endpoint:    <pre><code>curl http://localhost:11434/health\n</code></pre></p> </li> <li> <p>Check the metrics endpoint for performance data:    <pre><code>curl http://localhost:11434/metrics\n</code></pre></p> </li> </ol>"}]}